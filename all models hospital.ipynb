{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "train_url = \"https://raw.githubusercontent.com/zetomic/dataset/main/train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/zetomic/dataset/main/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_url)\n",
    "test_data = pd.read_csv(test_url)\n",
    "\n",
    "# One-hot encode categorical variables for train_data\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = encoder.fit_transform(train_data.select_dtypes(include=['object']))\n",
    "feature_names = encoder.get_feature_names(input_features=train_data.select_dtypes(include=['object']).columns)\n",
    "train_encoded = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "train_data = train_data.drop(train_data.select_dtypes(include=['object']).columns, axis=1)\n",
    "train_data = pd.concat([train_data, train_encoded], axis=1)\n",
    "\n",
    "# One-hot encode test data\n",
    "encoded_features_test = encoder.transform(test_data.select_dtypes(include=['object']))\n",
    "test_encoded = pd.DataFrame(encoded_features_test, columns=feature_names)\n",
    "test_data = test_data.drop(test_data.select_dtypes(include=['object']).columns, axis=1)\n",
    "test_data = pd.concat([test_data, test_encoded], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "train_data = train_data.fillna(train_data.median())\n",
    "test_data = test_data.fillna(train_data.median())\n",
    "\n",
    "# Ensure both train and test data have the same columns\n",
    "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
    "for c in missing_cols:\n",
    "    test_data[c] = 0\n",
    "test_data = test_data[train_data.columns.drop('hospital_death')]\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data.drop('hospital_death', axis=1), train_data['hospital_death'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Decision Tree with hyperparameter tuning\n",
    "param_grid_tree = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [3, 5, 7]\n",
    "}\n",
    "clf_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=5)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "print(\"Decision Tree Best Parameters:\", clf_tree.best_params_)\n",
    "y_val_pred_tree = clf_tree.predict(X_val)\n",
    "print(f\"Decision Tree Accuracy on validation set: {accuracy_score(y_val, y_val_pred_tree) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_tree))\n",
    "\n",
    "# Naive Bayes\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "print(\"\\nNaive Bayes Parameters:\", clf_nb.get_params())\n",
    "y_val_pred_nb = clf_nb.predict(X_val)\n",
    "print(f\"Naive Bayes Accuracy on validation set: {accuracy_score(y_val, y_val_pred_nb) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_nb))\n",
    "\n",
    "# KNN with hyperparameter tuning\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "clf_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)\n",
    "clf_knn.fit(X_train_scaled, y_train)\n",
    "print(\"\\nKNN Best Parameters:\", clf_knn.best_params_)\n",
    "y_val_pred_knn = clf_knn.predict(X_val_scaled)\n",
    "print(f\"KNN Accuracy on validation set: {accuracy_score(y_val, y_val_pred_knn) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_knn))\n",
    "\n",
    "print(\"\\nProbability predictions for test set with high precision are saved in respective CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load datasets\n",
    "train_url = \"https://raw.githubusercontent.com/zetomic/dataset/main/train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/zetomic/dataset/main/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_url)\n",
    "test_data = pd.read_csv(test_url)\n",
    "\n",
    "# One-hot encode categorical variables for train_data\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = encoder.fit_transform(train_data.select_dtypes(include=['object']))\n",
    "feature_names = encoder.get_feature_names(input_features=train_data.select_dtypes(include=['object']).columns)\n",
    "train_encoded = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "train_data = train_data.drop(train_data.select_dtypes(include=['object']).columns, axis=1)\n",
    "train_data = pd.concat([train_data, train_encoded], axis=1)\n",
    "\n",
    "# One-hot encode test data\n",
    "encoded_features_test = encoder.transform(test_data.select_dtypes(include=['object']))\n",
    "test_encoded = pd.DataFrame(encoded_features_test, columns=feature_names)\n",
    "test_data = test_data.drop(test_data.select_dtypes(include=['object']).columns, axis=1)\n",
    "test_data = pd.concat([test_data, test_encoded], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "train_data = train_data.fillna(train_data.median())\n",
    "test_data = test_data.fillna(train_data.median())\n",
    "\n",
    "# Ensure both train and test data have the same columns\n",
    "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
    "for c in missing_cols:\n",
    "    test_data[c] = 0\n",
    "test_data = test_data[train_data.columns.drop('hospital_death')]\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data.drop('hospital_death', axis=1), train_data['hospital_death'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Decision Tree with hyperparameter tuning\n",
    "param_grid_tree = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [3, 5, 7]\n",
    "}\n",
    "clf_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=5)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "print(\"Decision Tree Best Parameters:\", clf_tree.best_params_)\n",
    "y_val_pred_tree = clf_tree.predict(X_val)\n",
    "print(f\"Decision Tree Accuracy on validation set: {accuracy_score(y_val, y_val_pred_tree) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_tree))\n",
    "\n",
    "# Naive Bayes\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "print(\"\\nNaive Bayes Parameters:\", clf_nb.get_params())\n",
    "y_val_pred_nb = clf_nb.predict(X_val)\n",
    "print(f\"Naive Bayes Accuracy on validation set: {accuracy_score(y_val, y_val_pred_nb) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_nb))\n",
    "\n",
    "# KNN with hyperparameter tuning\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "clf_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)\n",
    "clf_knn.fit(X_train_scaled, y_train)\n",
    "print(\"\\nKNN Best Parameters:\", clf_knn.best_params_)\n",
    "y_val_pred_knn = clf_knn.predict(X_val_scaled)\n",
    "print(f\"KNN Accuracy on validation set: {accuracy_score(y_val, y_val_pred_knn) * 100:.2f}%\")\n",
    "print(classification_report(y_val, y_val_pred_knn))\n",
    "\n",
    "print(\"\\nProbability predictions for test set with high precision are saved in respective CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
