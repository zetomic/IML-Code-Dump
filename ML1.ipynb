{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'train.csv'  # Replace with your CSV file path\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting key features for the model\n",
    "key_features = ['full_sq', 'life_sq', 'floor', 'leisure_count_500', 'cafe_count_1000_price_high']\n",
    "data = data[key_features + ['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying logarithmic transformations\n",
    "data['full_sq_log'] = np.log1p(data['full_sq'])\n",
    "data['leisure_count_500_log'] = np.log1p(data['leisure_count_500'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling any potential infinities or NaNs\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = data.drop('price_doc', axis=1)\n",
    "y = data['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling using Robust Scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.01  # You can change this value as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Adjust the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Design\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),  # Increased dropout rate\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),  # Added dropout layer\n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model with a custom learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4538/4538 [==============================] - 5s 933us/step - loss: 223194300547072.0000 - rmse: 14939689.0000 - val_loss: 196073528229888.0000 - val_rmse: 14002626.0000\n",
      "Epoch 2/150\n",
      "4538/4538 [==============================] - 4s 844us/step - loss: 195823514157056.0000 - rmse: 13993696.0000 - val_loss: 194671808610304.0000 - val_rmse: 13952484.0000\n",
      "Epoch 3/150\n",
      "4538/4538 [==============================] - 4s 875us/step - loss: 193540923588608.0000 - rmse: 13911899.0000 - val_loss: 191394866003968.0000 - val_rmse: 13834553.0000\n",
      "Epoch 4/150\n",
      "4538/4538 [==============================] - 4s 854us/step - loss: 191489053294592.0000 - rmse: 13837957.0000 - val_loss: 188410132168704.0000 - val_rmse: 13726257.0000\n",
      "Epoch 5/150\n",
      "4538/4538 [==============================] - 4s 852us/step - loss: 189546000023552.0000 - rmse: 13767571.0000 - val_loss: 185703614906368.0000 - val_rmse: 13627311.0000\n",
      "Epoch 6/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 186735967338496.0000 - rmse: 13665137.0000 - val_loss: 182919284916224.0000 - val_rmse: 13524766.0000\n",
      "Epoch 7/150\n",
      "4538/4538 [==============================] - 4s 879us/step - loss: 184734529355776.0000 - rmse: 13591708.0000 - val_loss: 181027771252736.0000 - val_rmse: 13454656.0000\n",
      "Epoch 8/150\n",
      "4538/4538 [==============================] - 4s 857us/step - loss: 184097951449088.0000 - rmse: 13568270.0000 - val_loss: 180608709951488.0000 - val_rmse: 13439074.0000\n",
      "Epoch 9/150\n",
      "4538/4538 [==============================] - 4s 847us/step - loss: 182988776144896.0000 - rmse: 13527334.0000 - val_loss: 181961893085184.0000 - val_rmse: 13489325.0000\n",
      "Epoch 10/150\n",
      "4538/4538 [==============================] - 4s 866us/step - loss: 183225787875328.0000 - rmse: 13536092.0000 - val_loss: 179531142922240.0000 - val_rmse: 13398923.0000\n",
      "Epoch 11/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 182657308688384.0000 - rmse: 13515077.0000 - val_loss: 178447351545856.0000 - val_rmse: 13358419.0000\n",
      "Epoch 12/150\n",
      "4538/4538 [==============================] - 4s 860us/step - loss: 182259671891968.0000 - rmse: 13500358.0000 - val_loss: 177675146625024.0000 - val_rmse: 13329484.0000\n",
      "Epoch 13/150\n",
      "4538/4538 [==============================] - 4s 853us/step - loss: 182374780370944.0000 - rmse: 13504621.0000 - val_loss: 178508403834880.0000 - val_rmse: 13360704.0000\n",
      "Epoch 14/150\n",
      "4538/4538 [==============================] - 4s 859us/step - loss: 182188989480960.0000 - rmse: 13497740.0000 - val_loss: 177778947260416.0000 - val_rmse: 13333377.0000\n",
      "Epoch 15/150\n",
      "4538/4538 [==============================] - 4s 860us/step - loss: 181593297649664.0000 - rmse: 13475656.0000 - val_loss: 177067408752640.0000 - val_rmse: 13306668.0000\n",
      "Epoch 16/150\n",
      "4538/4538 [==============================] - 4s 864us/step - loss: 181506458779648.0000 - rmse: 13472433.0000 - val_loss: 176615497662464.0000 - val_rmse: 13289676.0000\n",
      "Epoch 17/150\n",
      "4538/4538 [==============================] - 4s 853us/step - loss: 180587805540352.0000 - rmse: 13438296.0000 - val_loss: 176338505826304.0000 - val_rmse: 13279251.0000\n",
      "Epoch 18/150\n",
      "4538/4538 [==============================] - 4s 857us/step - loss: 180683351785472.0000 - rmse: 13441851.0000 - val_loss: 175613277110272.0000 - val_rmse: 13251916.0000\n",
      "Epoch 19/150\n",
      "4538/4538 [==============================] - 4s 854us/step - loss: 179564999344128.0000 - rmse: 13400187.0000 - val_loss: 176107533893632.0000 - val_rmse: 13270551.0000\n",
      "Epoch 20/150\n",
      "4538/4538 [==============================] - 4s 847us/step - loss: 180458453204992.0000 - rmse: 13433483.0000 - val_loss: 177715277725696.0000 - val_rmse: 13330989.0000\n",
      "Epoch 21/150\n",
      "4538/4538 [==============================] - 4s 867us/step - loss: 179472120676352.0000 - rmse: 13396721.0000 - val_loss: 176168200306688.0000 - val_rmse: 13272837.0000\n",
      "Epoch 22/150\n",
      "4538/4538 [==============================] - 4s 852us/step - loss: 179656066072576.0000 - rmse: 13403584.0000 - val_loss: 174808071405568.0000 - val_rmse: 13221500.0000\n",
      "Epoch 23/150\n",
      "4538/4538 [==============================] - 4s 854us/step - loss: 178919110082560.0000 - rmse: 13376065.0000 - val_loss: 174931803373568.0000 - val_rmse: 13226179.0000\n",
      "Epoch 24/150\n",
      "4538/4538 [==============================] - 4s 860us/step - loss: 179429925978112.0000 - rmse: 13395146.0000 - val_loss: 177179463778304.0000 - val_rmse: 13310878.0000\n",
      "Epoch 25/150\n",
      "4538/4538 [==============================] - 4s 861us/step - loss: 178966270836736.0000 - rmse: 13377828.0000 - val_loss: 174128996810752.0000 - val_rmse: 13195795.0000\n",
      "Epoch 26/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 178563701538816.0000 - rmse: 13362773.0000 - val_loss: 174195854016512.0000 - val_rmse: 13198328.0000\n",
      "Epoch 27/150\n",
      "4538/4538 [==============================] - 4s 869us/step - loss: 179152430825472.0000 - rmse: 13384784.0000 - val_loss: 174907828731904.0000 - val_rmse: 13225272.0000\n",
      "Epoch 28/150\n",
      "4538/4538 [==============================] - 4s 864us/step - loss: 178254078017536.0000 - rmse: 13351183.0000 - val_loss: 174283783405568.0000 - val_rmse: 13201658.0000\n",
      "Epoch 29/150\n",
      "4538/4538 [==============================] - 4s 865us/step - loss: 179039989923840.0000 - rmse: 13380583.0000 - val_loss: 177123461431296.0000 - val_rmse: 13308774.0000\n",
      "Epoch 30/150\n",
      "4538/4538 [==============================] - 4s 851us/step - loss: 178312546615296.0000 - rmse: 13353372.0000 - val_loss: 174683584462848.0000 - val_rmse: 13216792.0000\n",
      "Epoch 31/150\n",
      "4538/4538 [==============================] - 4s 880us/step - loss: 178424953962496.0000 - rmse: 13357580.0000 - val_loss: 174508044451840.0000 - val_rmse: 13210149.0000\n",
      "Epoch 32/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 178501676171264.0000 - rmse: 13360452.0000 - val_loss: 173953825898496.0000 - val_rmse: 13189156.0000\n",
      "Epoch 33/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 178703556411392.0000 - rmse: 13368005.0000 - val_loss: 174215265255424.0000 - val_rmse: 13199063.0000\n",
      "Epoch 34/150\n",
      "4538/4538 [==============================] - 4s 884us/step - loss: 178749156884480.0000 - rmse: 13369710.0000 - val_loss: 174253349535744.0000 - val_rmse: 13200506.0000\n",
      "Epoch 35/150\n",
      "4538/4538 [==============================] - 4s 882us/step - loss: 178704529489920.0000 - rmse: 13368041.0000 - val_loss: 173823869583360.0000 - val_rmse: 13184228.0000\n",
      "Epoch 36/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 178552930566144.0000 - rmse: 13362370.0000 - val_loss: 173755116552192.0000 - val_rmse: 13181620.0000\n",
      "Epoch 37/150\n",
      "4538/4538 [==============================] - 4s 867us/step - loss: 178213359714304.0000 - rmse: 13349658.0000 - val_loss: 173847441571840.0000 - val_rmse: 13185122.0000\n",
      "Epoch 38/150\n",
      "4538/4538 [==============================] - 4s 932us/step - loss: 177964671041536.0000 - rmse: 13340340.0000 - val_loss: 173660073623552.0000 - val_rmse: 13178015.0000\n",
      "Epoch 39/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 178533385109504.0000 - rmse: 13361639.0000 - val_loss: 173364358414336.0000 - val_rmse: 13166790.0000\n",
      "Epoch 40/150\n",
      "4538/4538 [==============================] - 4s 900us/step - loss: 178158598881280.0000 - rmse: 13347606.0000 - val_loss: 174316666748928.0000 - val_rmse: 13202904.0000\n",
      "Epoch 41/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 178139120533504.0000 - rmse: 13346877.0000 - val_loss: 173156455153664.0000 - val_rmse: 13158893.0000\n",
      "Epoch 42/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 177549317505024.0000 - rmse: 13324763.0000 - val_loss: 172991249907712.0000 - val_rmse: 13152614.0000\n",
      "Epoch 43/150\n",
      "4538/4538 [==============================] - 4s 863us/step - loss: 177803072897024.0000 - rmse: 13334282.0000 - val_loss: 173293726334976.0000 - val_rmse: 13164108.0000\n",
      "Epoch 44/150\n",
      "4538/4538 [==============================] - 4s 859us/step - loss: 177946283212800.0000 - rmse: 13339651.0000 - val_loss: 174154951163904.0000 - val_rmse: 13196778.0000\n",
      "Epoch 45/150\n",
      "4538/4538 [==============================] - 4s 888us/step - loss: 177730326888448.0000 - rmse: 13331554.0000 - val_loss: 173222741934080.0000 - val_rmse: 13161411.0000\n",
      "Epoch 46/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 177686689349632.0000 - rmse: 13329917.0000 - val_loss: 172650152329216.0000 - val_rmse: 13139640.0000\n",
      "Epoch 47/150\n",
      "4538/4538 [==============================] - 4s 867us/step - loss: 177389145423872.0000 - rmse: 13318752.0000 - val_loss: 172785175363584.0000 - val_rmse: 13144777.0000\n",
      "Epoch 48/150\n",
      "4538/4538 [==============================] - 4s 902us/step - loss: 177784248860672.0000 - rmse: 13333576.0000 - val_loss: 172522410606592.0000 - val_rmse: 13134779.0000\n",
      "Epoch 49/150\n",
      "4538/4538 [==============================] - 4s 900us/step - loss: 177709976125440.0000 - rmse: 13330791.0000 - val_loss: 172590576435200.0000 - val_rmse: 13137373.0000\n",
      "Epoch 50/150\n",
      "4538/4538 [==============================] - 4s 888us/step - loss: 177211508260864.0000 - rmse: 13312081.0000 - val_loss: 172954423918592.0000 - val_rmse: 13151214.0000\n",
      "Epoch 51/150\n",
      "4538/4538 [==============================] - 4s 896us/step - loss: 176770116485120.0000 - rmse: 13295492.0000 - val_loss: 172991971328000.0000 - val_rmse: 13152641.0000\n",
      "Epoch 52/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 177228553912320.0000 - rmse: 13312722.0000 - val_loss: 172767441846272.0000 - val_rmse: 13144103.0000\n",
      "Epoch 53/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 176924953411584.0000 - rmse: 13301314.0000 - val_loss: 172432568614912.0000 - val_rmse: 13131358.0000\n",
      "Epoch 54/150\n",
      "4538/4538 [==============================] - 4s 883us/step - loss: 176967668203520.0000 - rmse: 13302920.0000 - val_loss: 172796516761600.0000 - val_rmse: 13145209.0000\n",
      "Epoch 55/150\n",
      "4538/4538 [==============================] - 4s 879us/step - loss: 177642951147520.0000 - rmse: 13328276.0000 - val_loss: 172824450826240.0000 - val_rmse: 13146271.0000\n",
      "Epoch 56/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 177435534426112.0000 - rmse: 13320493.0000 - val_loss: 172838778568704.0000 - val_rmse: 13146816.0000\n",
      "Epoch 57/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 176583553843200.0000 - rmse: 13288474.0000 - val_loss: 172271339569152.0000 - val_rmse: 13125218.0000\n",
      "Epoch 58/150\n",
      "4538/4538 [==============================] - 4s 875us/step - loss: 177030750535680.0000 - rmse: 13305290.0000 - val_loss: 172815827337216.0000 - val_rmse: 13145943.0000\n",
      "Epoch 59/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 177255582007296.0000 - rmse: 13313737.0000 - val_loss: 172937277603840.0000 - val_rmse: 13150562.0000\n",
      "Epoch 60/150\n",
      "4538/4538 [==============================] - 4s 896us/step - loss: 177075109494784.0000 - rmse: 13306957.0000 - val_loss: 172617755525120.0000 - val_rmse: 13138408.0000\n",
      "Epoch 61/150\n",
      "4538/4538 [==============================] - 4s 892us/step - loss: 176136155824128.0000 - rmse: 13271630.0000 - val_loss: 172634046201856.0000 - val_rmse: 13139028.0000\n",
      "Epoch 62/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 176613987713024.0000 - rmse: 13289620.0000 - val_loss: 174077608198144.0000 - val_rmse: 13193847.0000\n",
      "Epoch 63/150\n",
      "4538/4538 [==============================] - 4s 889us/step - loss: 176711094239232.0000 - rmse: 13293273.0000 - val_loss: 172308182335488.0000 - val_rmse: 13126621.0000\n",
      "Epoch 64/150\n",
      "4538/4538 [==============================] - 4s 880us/step - loss: 177522171969536.0000 - rmse: 13323745.0000 - val_loss: 172267682136064.0000 - val_rmse: 13125078.0000\n",
      "Epoch 65/150\n",
      "4538/4538 [==============================] - 4s 874us/step - loss: 176204690751488.0000 - rmse: 13274211.0000 - val_loss: 173814256238592.0000 - val_rmse: 13183863.0000\n",
      "Epoch 66/150\n",
      "4538/4538 [==============================] - 4s 885us/step - loss: 176749698613248.0000 - rmse: 13294724.0000 - val_loss: 172060416409600.0000 - val_rmse: 13117180.0000\n",
      "Epoch 67/150\n",
      "4538/4538 [==============================] - 4s 872us/step - loss: 176590717714432.0000 - rmse: 13288744.0000 - val_loss: 172509525704704.0000 - val_rmse: 13134288.0000\n",
      "Epoch 68/150\n",
      "4538/4538 [==============================] - 4s 893us/step - loss: 176959329927168.0000 - rmse: 13302606.0000 - val_loss: 171931600945152.0000 - val_rmse: 13112269.0000\n",
      "Epoch 69/150\n",
      "4538/4538 [==============================] - 4s 900us/step - loss: 176608837107712.0000 - rmse: 13289426.0000 - val_loss: 172374049685504.0000 - val_rmse: 13129130.0000\n",
      "Epoch 70/150\n",
      "4538/4538 [==============================] - 4s 884us/step - loss: 176729213632512.0000 - rmse: 13293954.0000 - val_loss: 172179115212800.0000 - val_rmse: 13121704.0000\n",
      "Epoch 71/150\n",
      "4538/4538 [==============================] - 4s 883us/step - loss: 176746443833344.0000 - rmse: 13294602.0000 - val_loss: 172187889696768.0000 - val_rmse: 13122038.0000\n",
      "Epoch 72/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 176621487128576.0000 - rmse: 13289902.0000 - val_loss: 176028328656896.0000 - val_rmse: 13267567.0000\n",
      "Epoch 73/150\n",
      "4538/4538 [==============================] - 4s 888us/step - loss: 176574192156672.0000 - rmse: 13288122.0000 - val_loss: 173751073243136.0000 - val_rmse: 13181467.0000\n",
      "Epoch 74/150\n",
      "4538/4538 [==============================] - 4s 889us/step - loss: 176213180022784.0000 - rmse: 13274531.0000 - val_loss: 173055322095616.0000 - val_rmse: 13155049.0000\n",
      "Epoch 75/150\n",
      "4538/4538 [==============================] - 4s 894us/step - loss: 176655444213760.0000 - rmse: 13291179.0000 - val_loss: 172724576059392.0000 - val_rmse: 13142472.0000\n",
      "Epoch 76/150\n",
      "4538/4538 [==============================] - 4s 905us/step - loss: 176853012709376.0000 - rmse: 13298609.0000 - val_loss: 173219218718720.0000 - val_rmse: 13161277.0000\n",
      "Epoch 77/150\n",
      "4538/4538 [==============================] - 4s 897us/step - loss: 176593251074048.0000 - rmse: 13288839.0000 - val_loss: 172648105508864.0000 - val_rmse: 13139563.0000\n",
      "Epoch 78/150\n",
      "4538/4538 [==============================] - 4s 866us/step - loss: 176352934232064.0000 - rmse: 13279794.0000 - val_loss: 171646354718720.0000 - val_rmse: 13101388.0000\n",
      "Epoch 79/150\n",
      "4538/4538 [==============================] - 4s 868us/step - loss: 176452926439424.0000 - rmse: 13283559.0000 - val_loss: 172535882711040.0000 - val_rmse: 13135291.0000\n",
      "Epoch 80/150\n",
      "4538/4538 [==============================] - 4s 854us/step - loss: 176522736435200.0000 - rmse: 13286186.0000 - val_loss: 172397823000576.0000 - val_rmse: 13130035.0000\n",
      "Epoch 81/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 176654538244096.0000 - rmse: 13291145.0000 - val_loss: 172156566634496.0000 - val_rmse: 13120845.0000\n",
      "Epoch 82/150\n",
      "4538/4538 [==============================] - 4s 883us/step - loss: 176701883547648.0000 - rmse: 13292926.0000 - val_loss: 172325446090752.0000 - val_rmse: 13127279.0000\n",
      "Epoch 83/150\n",
      "4538/4538 [==============================] - 4s 869us/step - loss: 176429018906624.0000 - rmse: 13282659.0000 - val_loss: 171796812791808.0000 - val_rmse: 13107128.0000\n",
      "Epoch 84/150\n",
      "4538/4538 [==============================] - 4s 875us/step - loss: 176455258472448.0000 - rmse: 13283646.0000 - val_loss: 172160156958720.0000 - val_rmse: 13120982.0000\n",
      "Epoch 85/150\n",
      "4538/4538 [==============================] - 4s 868us/step - loss: 176506110214144.0000 - rmse: 13285560.0000 - val_loss: 171652998496256.0000 - val_rmse: 13101641.0000\n",
      "Epoch 86/150\n",
      "4538/4538 [==============================] - 4s 894us/step - loss: 175930114834432.0000 - rmse: 13263865.0000 - val_loss: 171946952097792.0000 - val_rmse: 13112854.0000\n",
      "Epoch 87/150\n",
      "4538/4538 [==============================] - 4s 883us/step - loss: 176257236992000.0000 - rmse: 13276191.0000 - val_loss: 173032152760320.0000 - val_rmse: 13154169.0000\n",
      "Epoch 88/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 176255005622272.0000 - rmse: 13276107.0000 - val_loss: 171480478384128.0000 - val_rmse: 13095055.0000\n",
      "Epoch 89/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 176191570968576.0000 - rmse: 13273717.0000 - val_loss: 173494465724416.0000 - val_rmse: 13171730.0000\n",
      "Epoch 90/150\n",
      "4538/4538 [==============================] - 4s 889us/step - loss: 175794773032960.0000 - rmse: 13258762.0000 - val_loss: 172076388319232.0000 - val_rmse: 13117789.0000\n",
      "Epoch 91/150\n",
      "4538/4538 [==============================] - 4s 884us/step - loss: 175980765249536.0000 - rmse: 13265774.0000 - val_loss: 176601404801024.0000 - val_rmse: 13289146.0000\n",
      "Epoch 92/150\n",
      "4538/4538 [==============================] - 4s 885us/step - loss: 176364929941504.0000 - rmse: 13280246.0000 - val_loss: 173021985767424.0000 - val_rmse: 13153782.0000\n",
      "Epoch 93/150\n",
      "4538/4538 [==============================] - 4s 898us/step - loss: 175660840517632.0000 - rmse: 13253710.0000 - val_loss: 171574581788672.0000 - val_rmse: 13098648.0000\n",
      "Epoch 94/150\n",
      "4538/4538 [==============================] - 4s 860us/step - loss: 175908925210624.0000 - rmse: 13263066.0000 - val_loss: 172401044226048.0000 - val_rmse: 13130158.0000\n",
      "Epoch 95/150\n",
      "4538/4538 [==============================] - 4s 856us/step - loss: 176225226063872.0000 - rmse: 13274985.0000 - val_loss: 171653367595008.0000 - val_rmse: 13101655.0000\n",
      "Epoch 96/150\n",
      "4538/4538 [==============================] - 4s 893us/step - loss: 175840105070592.0000 - rmse: 13260472.0000 - val_loss: 172816766861312.0000 - val_rmse: 13145979.0000\n",
      "Epoch 97/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 176400162095104.0000 - rmse: 13281572.0000 - val_loss: 171562468638720.0000 - val_rmse: 13098186.0000\n",
      "Epoch 98/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 176445343137792.0000 - rmse: 13283273.0000 - val_loss: 172126317314048.0000 - val_rmse: 13119692.0000\n",
      "Epoch 99/150\n",
      "4538/4538 [==============================] - 4s 892us/step - loss: 176168217083904.0000 - rmse: 13272838.0000 - val_loss: 172321788657664.0000 - val_rmse: 13127139.0000\n",
      "Epoch 100/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 175923924041728.0000 - rmse: 13263632.0000 - val_loss: 174208386596864.0000 - val_rmse: 13198802.0000\n",
      "Epoch 101/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 175312092528640.0000 - rmse: 13240547.0000 - val_loss: 171429609865216.0000 - val_rmse: 13093113.0000\n",
      "Epoch 102/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 176012558073856.0000 - rmse: 13266972.0000 - val_loss: 171595200987136.0000 - val_rmse: 13099435.0000\n",
      "Epoch 103/150\n",
      "4538/4538 [==============================] - 4s 893us/step - loss: 176052940832768.0000 - rmse: 13268494.0000 - val_loss: 173741409566720.0000 - val_rmse: 13181100.0000\n",
      "Epoch 104/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 175880219394048.0000 - rmse: 13261984.0000 - val_loss: 171505610653696.0000 - val_rmse: 13096015.0000\n",
      "Epoch 105/150\n",
      "4538/4538 [==============================] - 4s 888us/step - loss: 176298408280064.0000 - rmse: 13277741.0000 - val_loss: 171578859978752.0000 - val_rmse: 13098811.0000\n",
      "Epoch 106/150\n",
      "4538/4538 [==============================] - 4s 874us/step - loss: 175534659076096.0000 - rmse: 13248949.0000 - val_loss: 171309132677120.0000 - val_rmse: 13088511.0000\n",
      "Epoch 107/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 174793609445376.0000 - rmse: 13220953.0000 - val_loss: 171753074589696.0000 - val_rmse: 13105460.0000\n",
      "Epoch 108/150\n",
      "4538/4538 [==============================] - 4s 929us/step - loss: 175869666525184.0000 - rmse: 13261586.0000 - val_loss: 171495879868416.0000 - val_rmse: 13095644.0000\n",
      "Epoch 109/150\n",
      "4538/4538 [==============================] - 4s 904us/step - loss: 175641093734400.0000 - rmse: 13252965.0000 - val_loss: 171229759668224.0000 - val_rmse: 13085479.0000\n",
      "Epoch 110/150\n",
      "4538/4538 [==============================] - 4s 855us/step - loss: 175477264220160.0000 - rmse: 13246783.0000 - val_loss: 171750558007296.0000 - val_rmse: 13105364.0000\n",
      "Epoch 111/150\n",
      "4538/4538 [==============================] - 4s 866us/step - loss: 175679329009664.0000 - rmse: 13254408.0000 - val_loss: 171666705481728.0000 - val_rmse: 13102164.0000\n",
      "Epoch 112/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 175722916216832.0000 - rmse: 13256052.0000 - val_loss: 171943294664704.0000 - val_rmse: 13112715.0000\n",
      "Epoch 113/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 175962863960064.0000 - rmse: 13265099.0000 - val_loss: 170960552460288.0000 - val_rmse: 13075188.0000\n",
      "Epoch 114/150\n",
      "4538/4538 [==============================] - 4s 864us/step - loss: 175598446051328.0000 - rmse: 13251356.0000 - val_loss: 171228434268160.0000 - val_rmse: 13085428.0000\n",
      "Epoch 115/150\n",
      "4538/4538 [==============================] - 4s 901us/step - loss: 175597825294336.0000 - rmse: 13251333.0000 - val_loss: 172177487822848.0000 - val_rmse: 13121642.0000\n",
      "Epoch 116/150\n",
      "4538/4538 [==============================] - 4s 873us/step - loss: 175169351974912.0000 - rmse: 13235156.0000 - val_loss: 174040849317888.0000 - val_rmse: 13192454.0000\n",
      "Epoch 117/150\n",
      "4538/4538 [==============================] - 4s 876us/step - loss: 175360813563904.0000 - rmse: 13242387.0000 - val_loss: 172230520602624.0000 - val_rmse: 13123663.0000\n",
      "Epoch 118/150\n",
      "4538/4538 [==============================] - 4s 896us/step - loss: 175988080115712.0000 - rmse: 13266050.0000 - val_loss: 173293256572928.0000 - val_rmse: 13164090.0000\n",
      "Epoch 119/150\n",
      "4538/4538 [==============================] - 4s 877us/step - loss: 175196732391424.0000 - rmse: 13236190.0000 - val_loss: 171957286862848.0000 - val_rmse: 13113249.0000\n",
      "Epoch 120/150\n",
      "4538/4538 [==============================] - 4s 875us/step - loss: 175539960676352.0000 - rmse: 13249149.0000 - val_loss: 172468740292608.0000 - val_rmse: 13132735.0000\n",
      "Epoch 121/150\n",
      "4538/4538 [==============================] - 4s 875us/step - loss: 175202906406912.0000 - rmse: 13236423.0000 - val_loss: 172537191333888.0000 - val_rmse: 13135341.0000\n",
      "Epoch 122/150\n",
      "4538/4538 [==============================] - 4s 901us/step - loss: 174943933300736.0000 - rmse: 13226637.0000 - val_loss: 175551620841472.0000 - val_rmse: 13249589.0000\n",
      "Epoch 123/150\n",
      "4538/4538 [==============================] - 4s 876us/step - loss: 175490316894208.0000 - rmse: 13247276.0000 - val_loss: 173058224553984.0000 - val_rmse: 13155160.0000\n",
      "Epoch 124/150\n",
      "4538/4538 [==============================] - 4s 892us/step - loss: 175143917715456.0000 - rmse: 13234195.0000 - val_loss: 171600234151936.0000 - val_rmse: 13099627.0000\n",
      "Epoch 125/150\n",
      "4538/4538 [==============================] - 4s 886us/step - loss: 175039697649664.0000 - rmse: 13230257.0000 - val_loss: 172177403936768.0000 - val_rmse: 13121639.0000\n",
      "Epoch 126/150\n",
      "4538/4538 [==============================] - 4s 905us/step - loss: 175129925517312.0000 - rmse: 13233666.0000 - val_loss: 171142300041216.0000 - val_rmse: 13082137.0000\n",
      "Epoch 127/150\n",
      "4538/4538 [==============================] - 4s 910us/step - loss: 174973142433792.0000 - rmse: 13227741.0000 - val_loss: 171295945785344.0000 - val_rmse: 13088008.0000\n",
      "Epoch 128/150\n",
      "4538/4538 [==============================] - 4s 880us/step - loss: 175075265347584.0000 - rmse: 13231601.0000 - val_loss: 171199644565504.0000 - val_rmse: 13084328.0000\n",
      "Epoch 129/150\n",
      "4538/4538 [==============================] - 4s 895us/step - loss: 174849427243008.0000 - rmse: 13223064.0000 - val_loss: 171373674627072.0000 - val_rmse: 13090977.0000\n",
      "Epoch 130/150\n",
      "4538/4538 [==============================] - 4s 866us/step - loss: 174748428402688.0000 - rmse: 13219245.0000 - val_loss: 172679864778752.0000 - val_rmse: 13140771.0000\n",
      "Epoch 131/150\n",
      "4538/4538 [==============================] - 4s 867us/step - loss: 174626793586688.0000 - rmse: 13214643.0000 - val_loss: 173211970961408.0000 - val_rmse: 13161002.0000\n",
      "Epoch 132/150\n",
      "4538/4538 [==============================] - 4s 845us/step - loss: 175319826825216.0000 - rmse: 13240839.0000 - val_loss: 172621446512640.0000 - val_rmse: 13138548.0000\n",
      "Epoch 133/150\n",
      "4538/4538 [==============================] - 4s 859us/step - loss: 175070483841024.0000 - rmse: 13231420.0000 - val_loss: 170869586395136.0000 - val_rmse: 13071709.0000\n",
      "Epoch 134/150\n",
      "4538/4538 [==============================] - 4s 878us/step - loss: 175191716003840.0000 - rmse: 13236001.0000 - val_loss: 171417496715264.0000 - val_rmse: 13092650.0000\n",
      "Epoch 135/150\n",
      "4538/4538 [==============================] - 4s 867us/step - loss: 174692577050624.0000 - rmse: 13217132.0000 - val_loss: 173852642508800.0000 - val_rmse: 13185319.0000\n",
      "Epoch 136/150\n",
      "4538/4538 [==============================] - 4s 907us/step - loss: 174814195089408.0000 - rmse: 13221732.0000 - val_loss: 172127055511552.0000 - val_rmse: 13119720.0000\n",
      "Epoch 137/150\n",
      "4538/4538 [==============================] - 4s 915us/step - loss: 174631558316032.0000 - rmse: 13214823.0000 - val_loss: 171027912982528.0000 - val_rmse: 13077764.0000\n",
      "Epoch 138/150\n",
      "4538/4538 [==============================] - 4s 894us/step - loss: 174915244261376.0000 - rmse: 13225553.0000 - val_loss: 171210130325504.0000 - val_rmse: 13084729.0000\n",
      "Epoch 139/150\n",
      "4538/4538 [==============================] - 4s 894us/step - loss: 174617599672320.0000 - rmse: 13214295.0000 - val_loss: 174807786192896.0000 - val_rmse: 13221490.0000\n",
      "Epoch 140/150\n",
      "4538/4538 [==============================] - 4s 892us/step - loss: 174406223527936.0000 - rmse: 13206295.0000 - val_loss: 172176917397504.0000 - val_rmse: 13121620.0000\n",
      "Epoch 141/150\n",
      "4538/4538 [==============================] - 4s 902us/step - loss: 174482039767040.0000 - rmse: 13209165.0000 - val_loss: 175408410525696.0000 - val_rmse: 13244184.0000\n",
      "Epoch 142/150\n",
      "4538/4538 [==============================] - 4s 919us/step - loss: 174742036283392.0000 - rmse: 13219003.0000 - val_loss: 174187918393344.0000 - val_rmse: 13198027.0000\n",
      "Epoch 143/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 175330513911808.0000 - rmse: 13241243.0000 - val_loss: 171164110422016.0000 - val_rmse: 13082970.0000\n",
      "Epoch 144/150\n",
      "4538/4538 [==============================] - 4s 891us/step - loss: 174763645337600.0000 - rmse: 13219820.0000 - val_loss: 172195691102208.0000 - val_rmse: 13122336.0000\n",
      "Epoch 145/150\n",
      "4538/4538 [==============================] - 4s 881us/step - loss: 174575455305728.0000 - rmse: 13212701.0000 - val_loss: 172323046948864.0000 - val_rmse: 13127187.0000\n",
      "Epoch 146/150\n",
      "4538/4538 [==============================] - 4s 889us/step - loss: 174588759638016.0000 - rmse: 13213204.0000 - val_loss: 171773207248896.0000 - val_rmse: 13106228.0000\n",
      "Epoch 147/150\n",
      "4538/4538 [==============================] - 4s 901us/step - loss: 174405049122816.0000 - rmse: 13206250.0000 - val_loss: 172959289311232.0000 - val_rmse: 13151399.0000\n",
      "Epoch 148/150\n",
      "4538/4538 [==============================] - 4s 887us/step - loss: 174084470079488.0000 - rmse: 13194107.0000 - val_loss: 171549499850752.0000 - val_rmse: 13097691.0000\n",
      "Epoch 149/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 174806410461184.0000 - rmse: 13221438.0000 - val_loss: 173665677213696.0000 - val_rmse: 13178227.0000\n",
      "Epoch 150/150\n",
      "4538/4538 [==============================] - 4s 890us/step - loss: 174774835740672.0000 - rmse: 13220243.0000 - val_loss: 174759165820928.0000 - val_rmse: 13219651.0000\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with the specified batch size\n",
    "history = model.fit(X_train_scaled, y_train, batch_size=32, epochs=150, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135/1135 [==============================] - 1s 589us/step - loss: 174759165820928.0000 - rmse: 13219651.0000\n",
      "Validation RMSE: 13219651.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "results = model.evaluate(X_val_scaled, y_val)\n",
    "print(f\"Validation RMSE: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_file_path = 'test.csv'  # Replace with your test CSV file path\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Apply the same preprocessing steps to the test dataset\n",
    "test_data['full_sq_log'] = np.log1p(test_data['full_sq'])\n",
    "test_data['leisure_count_500_log'] = np.log1p(test_data['leisure_count_500'])\n",
    "\n",
    "# Handle potential infinities or NaNs\n",
    "test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "# Select the same features used for training (including original and transformed features)\n",
    "X_test = test_data[['full_sq', 'life_sq', 'floor', 'leisure_count_500', 'cafe_count_1000_price_high', 'full_sq_log', 'leisure_count_500_log']]\n",
    "\n",
    "# Scale the test data using the same scaler as the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "submission = pd.DataFrame({\n",
    "    'row ID': test_data['row ID'],  # Replace 'row ID' with the actual identifier column of your test dataset\n",
    "    'price_doc': predictions.flatten()\n",
    "})\n",
    "submission.to_csv('neural-network.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
