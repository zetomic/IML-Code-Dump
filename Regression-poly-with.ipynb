{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'train.csv'  # Replace with your CSV file path\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting key features for the model\n",
    "key_features = ['full_sq', 'life_sq', 'floor', 'leisure_count_500', 'cafe_count_1000_price_high']\n",
    "data = data[key_features + ['price_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying logarithmic transformations\n",
    "data['full_sq_log'] = np.log1p(data['full_sq'])\n",
    "data['leisure_count_500_log'] = np.log1p(data['leisure_count_500'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling any potential infinities or NaNs\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = data.drop('price_doc', axis=1)\n",
    "y = data['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling using Robust Scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.01  # You can change this value as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Adjust the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Design\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.4),  # Increased dropout rate\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  # Added dropout layer\n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model with a custom learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 229722717945856.0000 - rmse: 15156606.0000 - val_loss: 191354198032384.0000 - val_rmse: 13833083.0000\n",
      "Epoch 2/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 191726887108608.0000 - rmse: 13846548.0000 - val_loss: 184593600741376.0000 - val_rmse: 13586523.0000\n",
      "Epoch 3/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 187976877342720.0000 - rmse: 13710466.0000 - val_loss: 181316272259072.0000 - val_rmse: 13465373.0000\n",
      "Epoch 4/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 186265248989184.0000 - rmse: 13647903.0000 - val_loss: 178981303222272.0000 - val_rmse: 13378389.0000\n",
      "Epoch 5/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 184712031109120.0000 - rmse: 13590880.0000 - val_loss: 177381998329856.0000 - val_rmse: 13318483.0000\n",
      "Epoch 6/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 182959030140928.0000 - rmse: 13526235.0000 - val_loss: 177754569965568.0000 - val_rmse: 13332463.0000\n",
      "Epoch 7/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 182517000830976.0000 - rmse: 13509885.0000 - val_loss: 175803681734656.0000 - val_rmse: 13259098.0000\n",
      "Epoch 8/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 181966842363904.0000 - rmse: 13489509.0000 - val_loss: 176234671636480.0000 - val_rmse: 13275341.0000\n",
      "Epoch 9/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 181375512608768.0000 - rmse: 13467573.0000 - val_loss: 174526079959040.0000 - val_rmse: 13210832.0000\n",
      "Epoch 10/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 181046628843520.0000 - rmse: 13455357.0000 - val_loss: 173818282770432.0000 - val_rmse: 13184016.0000\n",
      "Epoch 11/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 180742827016192.0000 - rmse: 13444063.0000 - val_loss: 175286977036288.0000 - val_rmse: 13239599.0000\n",
      "Epoch 12/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179740052815872.0000 - rmse: 13406717.0000 - val_loss: 173101459439616.0000 - val_rmse: 13156803.0000\n",
      "Epoch 13/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179185968480256.0000 - rmse: 13386036.0000 - val_loss: 173162562060288.0000 - val_rmse: 13159125.0000\n",
      "Epoch 14/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179926867116032.0000 - rmse: 13413682.0000 - val_loss: 174781026533376.0000 - val_rmse: 13220478.0000\n",
      "Epoch 15/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 178981403885568.0000 - rmse: 13378393.0000 - val_loss: 176415462916096.0000 - val_rmse: 13282148.0000\n",
      "Epoch 16/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 180134703267840.0000 - rmse: 13421427.0000 - val_loss: 172534104326144.0000 - val_rmse: 13135224.0000\n",
      "Epoch 17/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179192847138816.0000 - rmse: 13386293.0000 - val_loss: 174246202441728.0000 - val_rmse: 13200235.0000\n",
      "Epoch 18/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178961237671936.0000 - rmse: 13377639.0000 - val_loss: 172464629874688.0000 - val_rmse: 13132579.0000\n",
      "Epoch 19/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179344311844864.0000 - rmse: 13391950.0000 - val_loss: 172976402071552.0000 - val_rmse: 13152049.0000\n",
      "Epoch 20/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178428980494336.0000 - rmse: 13357731.0000 - val_loss: 173724263251968.0000 - val_rmse: 13180450.0000\n",
      "Epoch 21/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 179339865882624.0000 - rmse: 13391784.0000 - val_loss: 171925376598016.0000 - val_rmse: 13112032.0000\n",
      "Epoch 22/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178901510782976.0000 - rmse: 13375407.0000 - val_loss: 172037247074304.0000 - val_rmse: 13116297.0000\n",
      "Epoch 23/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 178524828729344.0000 - rmse: 13361318.0000 - val_loss: 172230503825408.0000 - val_rmse: 13123662.0000\n",
      "Epoch 24/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178206917263360.0000 - rmse: 13349416.0000 - val_loss: 172408124211200.0000 - val_rmse: 13130427.0000\n",
      "Epoch 25/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178219684724736.0000 - rmse: 13349895.0000 - val_loss: 172934425477120.0000 - val_rmse: 13150453.0000\n",
      "Epoch 26/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178310197805056.0000 - rmse: 13353284.0000 - val_loss: 171903616548864.0000 - val_rmse: 13111202.0000\n",
      "Epoch 27/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178513940316160.0000 - rmse: 13360911.0000 - val_loss: 174344382709760.0000 - val_rmse: 13203953.0000\n",
      "Epoch 28/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178779557199872.0000 - rmse: 13370847.0000 - val_loss: 171898465943552.0000 - val_rmse: 13111006.0000\n",
      "Epoch 29/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177993863397376.0000 - rmse: 13341434.0000 - val_loss: 172282748076032.0000 - val_rmse: 13125652.0000\n",
      "Epoch 30/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178427319549952.0000 - rmse: 13357669.0000 - val_loss: 173523943292928.0000 - val_rmse: 13172849.0000\n",
      "Epoch 31/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178512581361664.0000 - rmse: 13360860.0000 - val_loss: 171967034425344.0000 - val_rmse: 13113620.0000\n",
      "Epoch 32/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177875533692928.0000 - rmse: 13336999.0000 - val_loss: 172287479250944.0000 - val_rmse: 13125833.0000\n",
      "Epoch 33/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178129758846976.0000 - rmse: 13346526.0000 - val_loss: 172278369222656.0000 - val_rmse: 13125485.0000\n",
      "Epoch 34/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177857078755328.0000 - rmse: 13336307.0000 - val_loss: 172228809326592.0000 - val_rmse: 13123597.0000\n",
      "Epoch 35/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178394889191424.0000 - rmse: 13356455.0000 - val_loss: 172843039981568.0000 - val_rmse: 13146978.0000\n",
      "Epoch 36/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178506675781632.0000 - rmse: 13360639.0000 - val_loss: 174297423282176.0000 - val_rmse: 13202175.0000\n",
      "Epoch 37/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177501502439424.0000 - rmse: 13322969.0000 - val_loss: 171654592331776.0000 - val_rmse: 13101702.0000\n",
      "Epoch 38/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177528094326784.0000 - rmse: 13323967.0000 - val_loss: 171938966142976.0000 - val_rmse: 13112550.0000\n",
      "Epoch 39/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177502475517952.0000 - rmse: 13323005.0000 - val_loss: 171642462404608.0000 - val_rmse: 13101239.0000\n",
      "Epoch 40/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177766045581312.0000 - rmse: 13332893.0000 - val_loss: 172568195629056.0000 - val_rmse: 13136521.0000\n",
      "Epoch 41/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177412465754112.0000 - rmse: 13319627.0000 - val_loss: 171414711697408.0000 - val_rmse: 13092544.0000\n",
      "Epoch 42/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 178047013617664.0000 - rmse: 13343426.0000 - val_loss: 174251269160960.0000 - val_rmse: 13200427.0000\n",
      "Epoch 43/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 177721720176640.0000 - rmse: 13331231.0000 - val_loss: 171747219341312.0000 - val_rmse: 13105236.0000\n",
      "Epoch 44/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177610755670016.0000 - rmse: 13327069.0000 - val_loss: 174218570366976.0000 - val_rmse: 13199188.0000\n",
      "Epoch 45/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177999181774848.0000 - rmse: 13341633.0000 - val_loss: 172853659959296.0000 - val_rmse: 13147382.0000\n",
      "Epoch 46/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 177303514513408.0000 - rmse: 13315537.0000 - val_loss: 171693918126080.0000 - val_rmse: 13103203.0000\n",
      "Epoch 47/150\n",
      "4538/4538 [==============================] - 11s 2ms/step - loss: 177167115747328.0000 - rmse: 13310414.0000 - val_loss: 171619427287040.0000 - val_rmse: 13100360.0000\n",
      "Epoch 48/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177545458745344.0000 - rmse: 13324619.0000 - val_loss: 171366376538112.0000 - val_rmse: 13090698.0000\n",
      "Epoch 49/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177599565266944.0000 - rmse: 13326649.0000 - val_loss: 171651555655680.0000 - val_rmse: 13101586.0000\n",
      "Epoch 50/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177699742023680.0000 - rmse: 13330407.0000 - val_loss: 171979667668992.0000 - val_rmse: 13114102.0000\n",
      "Epoch 51/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177224896479232.0000 - rmse: 13312584.0000 - val_loss: 172961789116416.0000 - val_rmse: 13151494.0000\n",
      "Epoch 52/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177252696326144.0000 - rmse: 13313628.0000 - val_loss: 173295370502144.0000 - val_rmse: 13164170.0000\n",
      "Epoch 53/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176869471158272.0000 - rmse: 13299228.0000 - val_loss: 174837431533568.0000 - val_rmse: 13222611.0000\n",
      "Epoch 54/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177333025636352.0000 - rmse: 13316645.0000 - val_loss: 172067949379584.0000 - val_rmse: 13117467.0000\n",
      "Epoch 55/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177923348758528.0000 - rmse: 13338791.0000 - val_loss: 171679758155776.0000 - val_rmse: 13102662.0000\n",
      "Epoch 56/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177620671004672.0000 - rmse: 13327441.0000 - val_loss: 171868736716800.0000 - val_rmse: 13109872.0000\n",
      "Epoch 57/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177137168416768.0000 - rmse: 13309289.0000 - val_loss: 171498060906496.0000 - val_rmse: 13095727.0000\n",
      "Epoch 58/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177885784571904.0000 - rmse: 13337383.0000 - val_loss: 171943395328000.0000 - val_rmse: 13112719.0000\n",
      "Epoch 59/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176939633475584.0000 - rmse: 13301866.0000 - val_loss: 171225045270528.0000 - val_rmse: 13085299.0000\n",
      "Epoch 60/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177749067038720.0000 - rmse: 13332257.0000 - val_loss: 172206696955904.0000 - val_rmse: 13122755.0000\n",
      "Epoch 61/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177218940567552.0000 - rmse: 13312360.0000 - val_loss: 172598042296320.0000 - val_rmse: 13137657.0000\n",
      "Epoch 62/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177904088514560.0000 - rmse: 13338069.0000 - val_loss: 171537353146368.0000 - val_rmse: 13097227.0000\n",
      "Epoch 63/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176843198038016.0000 - rmse: 13298240.0000 - val_loss: 172995192553472.0000 - val_rmse: 13152764.0000\n",
      "Epoch 64/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176726831267840.0000 - rmse: 13293864.0000 - val_loss: 172164602920960.0000 - val_rmse: 13121151.0000\n",
      "Epoch 65/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177708432621568.0000 - rmse: 13330733.0000 - val_loss: 172023506534400.0000 - val_rmse: 13115773.0000\n",
      "Epoch 66/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176988237070336.0000 - rmse: 13303693.0000 - val_loss: 171448182243328.0000 - val_rmse: 13093822.0000\n",
      "Epoch 67/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 177072307699712.0000 - rmse: 13306852.0000 - val_loss: 171325725343744.0000 - val_rmse: 13089145.0000\n",
      "Epoch 68/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176854522658816.0000 - rmse: 13298666.0000 - val_loss: 171272810004480.0000 - val_rmse: 13087124.0000\n",
      "Epoch 69/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177009275699200.0000 - rmse: 13304483.0000 - val_loss: 171307220074496.0000 - val_rmse: 13088438.0000\n",
      "Epoch 70/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177661691297792.0000 - rmse: 13328979.0000 - val_loss: 174692006625280.0000 - val_rmse: 13217110.0000\n",
      "Epoch 71/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176573017751552.0000 - rmse: 13288078.0000 - val_loss: 171535541207040.0000 - val_rmse: 13097158.0000\n",
      "Epoch 72/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177263937060864.0000 - rmse: 13314050.0000 - val_loss: 177789584015360.0000 - val_rmse: 13333776.0000\n",
      "Epoch 73/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177089252687872.0000 - rmse: 13307489.0000 - val_loss: 171189427240960.0000 - val_rmse: 13083938.0000\n",
      "Epoch 74/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176773488705536.0000 - rmse: 13295619.0000 - val_loss: 172392924053504.0000 - val_rmse: 13129849.0000\n",
      "Epoch 75/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177308681895936.0000 - rmse: 13315731.0000 - val_loss: 171619360178176.0000 - val_rmse: 13100357.0000\n",
      "Epoch 76/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176942418493440.0000 - rmse: 13301970.0000 - val_loss: 172361282224128.0000 - val_rmse: 13128644.0000\n",
      "Epoch 77/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176865729839104.0000 - rmse: 13299088.0000 - val_loss: 172257833910272.0000 - val_rmse: 13124703.0000\n",
      "Epoch 78/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177479088078848.0000 - rmse: 13322128.0000 - val_loss: 171302153355264.0000 - val_rmse: 13088245.0000\n",
      "Epoch 79/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176783991242752.0000 - rmse: 13296014.0000 - val_loss: 171156912996352.0000 - val_rmse: 13082695.0000\n",
      "Epoch 80/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177340625715200.0000 - rmse: 13316930.0000 - val_loss: 171336664088576.0000 - val_rmse: 13089563.0000\n",
      "Epoch 81/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176633466060800.0000 - rmse: 13290352.0000 - val_loss: 171717624332288.0000 - val_rmse: 13104107.0000\n",
      "Epoch 82/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176905709944832.0000 - rmse: 13300591.0000 - val_loss: 170913408483328.0000 - val_rmse: 13073386.0000\n",
      "Epoch 83/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177127622180864.0000 - rmse: 13308930.0000 - val_loss: 173819088076800.0000 - val_rmse: 13184047.0000\n",
      "Epoch 84/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176688042344448.0000 - rmse: 13292405.0000 - val_loss: 171093008580608.0000 - val_rmse: 13080253.0000\n",
      "Epoch 85/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177162804002816.0000 - rmse: 13310252.0000 - val_loss: 173094496894976.0000 - val_rmse: 13156538.0000\n",
      "Epoch 86/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176711295565824.0000 - rmse: 13293280.0000 - val_loss: 174224912154624.0000 - val_rmse: 13199428.0000\n",
      "Epoch 87/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176593955717120.0000 - rmse: 13288866.0000 - val_loss: 174284135727104.0000 - val_rmse: 13201672.0000\n",
      "Epoch 88/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177094369738752.0000 - rmse: 13307681.0000 - val_loss: 174435365552128.0000 - val_rmse: 13207398.0000\n",
      "Epoch 89/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176523608850432.0000 - rmse: 13286219.0000 - val_loss: 171151762391040.0000 - val_rmse: 13082498.0000\n",
      "Epoch 90/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176865411072000.0000 - rmse: 13299076.0000 - val_loss: 170851416670208.0000 - val_rmse: 13071014.0000\n",
      "Epoch 91/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177184765378560.0000 - rmse: 13311077.0000 - val_loss: 171157433090048.0000 - val_rmse: 13082715.0000\n",
      "Epoch 92/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176716110626816.0000 - rmse: 13293461.0000 - val_loss: 173573687738368.0000 - val_rmse: 13174737.0000\n",
      "Epoch 93/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176855579623424.0000 - rmse: 13298706.0000 - val_loss: 172874799251456.0000 - val_rmse: 13148186.0000\n",
      "Epoch 94/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177142251913216.0000 - rmse: 13309480.0000 - val_loss: 170889668722688.0000 - val_rmse: 13072478.0000\n",
      "Epoch 95/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 175728721133568.0000 - rmse: 13256271.0000 - val_loss: 172183510843392.0000 - val_rmse: 13121871.0000\n",
      "Epoch 96/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176469770764288.0000 - rmse: 13284193.0000 - val_loss: 175589151473664.0000 - val_rmse: 13251006.0000\n",
      "Epoch 97/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 176898126643200.0000 - rmse: 13300306.0000 - val_loss: 170903073718272.0000 - val_rmse: 13072990.0000\n",
      "Epoch 98/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 176107936546816.0000 - rmse: 13270567.0000 - val_loss: 171392012124160.0000 - val_rmse: 13091677.0000\n",
      "Epoch 99/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176715322097664.0000 - rmse: 13293432.0000 - val_loss: 171868552167424.0000 - val_rmse: 13109865.0000\n",
      "Epoch 100/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176778270212096.0000 - rmse: 13295799.0000 - val_loss: 175761856135168.0000 - val_rmse: 13257521.0000\n",
      "Epoch 101/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 176957417324544.0000 - rmse: 13302534.0000 - val_loss: 170648345247744.0000 - val_rmse: 13063244.0000\n",
      "Epoch 102/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176707352920064.0000 - rmse: 13293132.0000 - val_loss: 172409550274560.0000 - val_rmse: 13130482.0000\n",
      "Epoch 103/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176904552316928.0000 - rmse: 13300547.0000 - val_loss: 171198705041408.0000 - val_rmse: 13084292.0000\n",
      "Epoch 104/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176386958426112.0000 - rmse: 13281075.0000 - val_loss: 172046373879808.0000 - val_rmse: 13116645.0000\n",
      "Epoch 105/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176507536277504.0000 - rmse: 13285614.0000 - val_loss: 171834343424000.0000 - val_rmse: 13108560.0000\n",
      "Epoch 106/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176694166028288.0000 - rmse: 13292636.0000 - val_loss: 171131344519168.0000 - val_rmse: 13081718.0000\n",
      "Epoch 107/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176819240173568.0000 - rmse: 13297340.0000 - val_loss: 171173522440192.0000 - val_rmse: 13083330.0000\n",
      "Epoch 108/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176305320493056.0000 - rmse: 13278001.0000 - val_loss: 170481093181440.0000 - val_rmse: 13056841.0000\n",
      "Epoch 109/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176389860884480.0000 - rmse: 13281184.0000 - val_loss: 173125954174976.0000 - val_rmse: 13157734.0000\n",
      "Epoch 110/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176600599494656.0000 - rmse: 13289116.0000 - val_loss: 171602901729280.0000 - val_rmse: 13099729.0000\n",
      "Epoch 111/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176553069641728.0000 - rmse: 13287327.0000 - val_loss: 171656890810368.0000 - val_rmse: 13101790.0000\n",
      "Epoch 112/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176324916281344.0000 - rmse: 13278739.0000 - val_loss: 171382717546496.0000 - val_rmse: 13091322.0000\n",
      "Epoch 113/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177060429430784.0000 - rmse: 13306406.0000 - val_loss: 171363843178496.0000 - val_rmse: 13090601.0000\n",
      "Epoch 114/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 177152083361792.0000 - rmse: 13309849.0000 - val_loss: 171167952404480.0000 - val_rmse: 13083117.0000\n",
      "Epoch 115/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176194624421888.0000 - rmse: 13273832.0000 - val_loss: 170871515774976.0000 - val_rmse: 13071783.0000\n",
      "Epoch 116/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176404389953536.0000 - rmse: 13281731.0000 - val_loss: 170567361626112.0000 - val_rmse: 13060144.0000\n",
      "Epoch 117/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176561122705408.0000 - rmse: 13287630.0000 - val_loss: 170680440061952.0000 - val_rmse: 13064472.0000\n",
      "Epoch 118/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 177059020144640.0000 - rmse: 13306353.0000 - val_loss: 171272407351296.0000 - val_rmse: 13087108.0000\n",
      "Epoch 119/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176141524533248.0000 - rmse: 13271832.0000 - val_loss: 170686244978688.0000 - val_rmse: 13064695.0000\n",
      "Epoch 120/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176329311911936.0000 - rmse: 13278905.0000 - val_loss: 175152054665216.0000 - val_rmse: 13234502.0000\n",
      "Epoch 121/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176540620947456.0000 - rmse: 13286859.0000 - val_loss: 171241688268800.0000 - val_rmse: 13085935.0000\n",
      "Epoch 122/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176250794541056.0000 - rmse: 13275948.0000 - val_loss: 172568631836672.0000 - val_rmse: 13136538.0000\n",
      "Epoch 123/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176952904253440.0000 - rmse: 13302365.0000 - val_loss: 171266384330752.0000 - val_rmse: 13086878.0000\n",
      "Epoch 124/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176528390356992.0000 - rmse: 13286399.0000 - val_loss: 171421372252160.0000 - val_rmse: 13092798.0000\n",
      "Epoch 125/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176245593604096.0000 - rmse: 13275752.0000 - val_loss: 171492172103680.0000 - val_rmse: 13095502.0000\n",
      "Epoch 126/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176051263111168.0000 - rmse: 13268431.0000 - val_loss: 171174965280768.0000 - val_rmse: 13083385.0000\n",
      "Epoch 127/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 175920954474496.0000 - rmse: 13263520.0000 - val_loss: 171350673063936.0000 - val_rmse: 13090098.0000\n",
      "Epoch 128/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176063426592768.0000 - rmse: 13268889.0000 - val_loss: 173369156698112.0000 - val_rmse: 13166972.0000\n",
      "Epoch 129/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176188819505152.0000 - rmse: 13273614.0000 - val_loss: 172779185897472.0000 - val_rmse: 13144550.0000\n",
      "Epoch 130/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176340938522624.0000 - rmse: 13279343.0000 - val_loss: 172168881111040.0000 - val_rmse: 13121314.0000\n",
      "Epoch 131/150\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 176327046987776.0000 - rmse: 13278819.0000 - val_loss: 170876112732160.0000 - val_rmse: 13071959.0000\n",
      "Epoch 132/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176412644343808.0000 - rmse: 13282042.0000 - val_loss: 170717534486528.0000 - val_rmse: 13065892.0000\n",
      "Epoch 133/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176092132409344.0000 - rmse: 13269971.0000 - val_loss: 170771003473920.0000 - val_rmse: 13067938.0000\n",
      "Epoch 134/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176447943606272.0000 - rmse: 13283371.0000 - val_loss: 170794927783936.0000 - val_rmse: 13068853.0000\n",
      "Epoch 135/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176561877680128.0000 - rmse: 13287659.0000 - val_loss: 172817303732224.0000 - val_rmse: 13146000.0000\n",
      "Epoch 136/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176152245174272.0000 - rmse: 13272236.0000 - val_loss: 172660185104384.0000 - val_rmse: 13140022.0000\n",
      "Epoch 137/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176800936230912.0000 - rmse: 13296651.0000 - val_loss: 170609505992704.0000 - val_rmse: 13061757.0000\n",
      "Epoch 138/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 175967981010944.0000 - rmse: 13265292.0000 - val_loss: 174017025671168.0000 - val_rmse: 13191551.0000\n",
      "Epoch 139/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 175925819867136.0000 - rmse: 13263703.0000 - val_loss: 170655894994944.0000 - val_rmse: 13063533.0000\n",
      "Epoch 140/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 175933738713088.0000 - rmse: 13264002.0000 - val_loss: 170515469697024.0000 - val_rmse: 13058157.0000\n",
      "Epoch 141/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176235728601088.0000 - rmse: 13275381.0000 - val_loss: 170271562530816.0000 - val_rmse: 13048815.0000\n",
      "Epoch 142/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 175921491345408.0000 - rmse: 13263540.0000 - val_loss: 172052313014272.0000 - val_rmse: 13116871.0000\n",
      "Epoch 143/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176685592870912.0000 - rmse: 13292313.0000 - val_loss: 171201221623808.0000 - val_rmse: 13084388.0000\n",
      "Epoch 144/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176297972072448.0000 - rmse: 13277725.0000 - val_loss: 170831082684416.0000 - val_rmse: 13070237.0000\n",
      "Epoch 145/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176087887773696.0000 - rmse: 13269811.0000 - val_loss: 173601621803008.0000 - val_rmse: 13175797.0000\n",
      "Epoch 146/150\n",
      "4538/4538 [==============================] - 8s 2ms/step - loss: 176461398933504.0000 - rmse: 13283877.0000 - val_loss: 173996859457536.0000 - val_rmse: 13190787.0000\n",
      "Epoch 147/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176617913581568.0000 - rmse: 13289767.0000 - val_loss: 170693509513216.0000 - val_rmse: 13064973.0000\n",
      "Epoch 148/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176537315835904.0000 - rmse: 13286735.0000 - val_loss: 170375379943424.0000 - val_rmse: 13052792.0000\n",
      "Epoch 149/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176105134751744.0000 - rmse: 13270461.0000 - val_loss: 172284711010304.0000 - val_rmse: 13125727.0000\n",
      "Epoch 150/150\n",
      "4538/4538 [==============================] - 9s 2ms/step - loss: 176194506981376.0000 - rmse: 13273828.0000 - val_loss: 172082780438528.0000 - val_rmse: 13118033.0000\n"
     ]
    }
   ],
   "source": [
    "# Training the Model with the specified batch size\n",
    "history = model.fit(X_train_scaled, y_train, batch_size=32, epochs=150, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               2048      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35073 (137.00 KB)\n",
      "Trainable params: 35073 (137.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135/1135 [==============================] - 1s 1ms/step - loss: 172082780438528.0000 - rmse: 13118033.0000\n",
      "Validation RMSE: 13118033.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "results = model.evaluate(X_val_scaled, y_val)\n",
    "print(f\"Validation RMSE: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431/2431 [==============================] - 2s 976us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_file_path = 'test.csv'  # Replace with your test CSV file path\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# Apply the same preprocessing steps to the test dataset\n",
    "test_data['full_sq_log'] = np.log1p(test_data['full_sq'])\n",
    "test_data['leisure_count_500_log'] = np.log1p(test_data['leisure_count_500'])\n",
    "\n",
    "# Handle potential infinities or NaNs\n",
    "test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "# Select the same features used for training (including original and transformed features)\n",
    "X_test = test_data[['full_sq', 'life_sq', 'floor', 'leisure_count_500', 'cafe_count_1000_price_high', 'full_sq_log', 'leisure_count_500_log']]\n",
    "\n",
    "# Scale the test data using the same scaler as the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "submission = pd.DataFrame({\n",
    "    'row ID': test_data['row ID'],  # Replace 'row ID' with the actual identifier column of your test dataset\n",
    "    'price_doc': predictions.flatten()\n",
    "})\n",
    "submission.to_csv('neural-network-2.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
